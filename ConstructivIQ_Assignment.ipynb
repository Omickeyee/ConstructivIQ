{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MDDvue3qRpBx"
      },
      "outputs": [],
      "source": [
        "# Importing necessary Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_urQFRaRtZM",
        "outputId": "6f689cef-626f-44d9-eee1-ded4a0d02f5d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data\n",
        "materials = pd.read_csv('materials.csv')\n",
        "ground_truth = pd.read_csv('submission.csv')\n",
        "test_pairs = pd.read_csv('test_pairs.csv')"
      ],
      "metadata": {
        "id": "-smbeCZMRtcl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase and remove special characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', str(text).lower())\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "materials['cleaned_description'] = materials['Material_Description'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "Qu3sr0qjSU5F"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TF-IDF vectors with parameters\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=2,\n",
        "    max_df=0.95\n",
        ")\n",
        "tfidf_matrix = vectorizer.fit_transform(materials['cleaned_description'])"
      ],
      "metadata": {
        "id": "l4stgF9ZSU73"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate similarity between two material IDs\n",
        "def calculate_similarity(id1, id2):\n",
        "    vec1 = tfidf_matrix[materials[materials['ID'] == id1].index[0]]\n",
        "    vec2 = tfidf_matrix[materials[materials['ID'] == id2].index[0]]\n",
        "    return cosine_similarity(vec1, vec2)[0][0]\n"
      ],
      "metadata": {
        "id": "1nNAFHUxSU_Q"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate similarities for ground truth data\n",
        "ground_truth['Predicted_Similarity'] = ground_truth.apply(lambda row: calculate_similarity(row['ID_1'], row['ID_2']), axis=1)\n",
        "\n",
        "# MAP@K implementation\n",
        "def apk(actual, predicted, k=10):\n",
        "    if not actual:\n",
        "        return 0.0\n",
        "    if len(predicted) > k:\n",
        "        predicted = predicted[:k]\n",
        "    score = 0.0\n",
        "    num_hits = 0.0\n",
        "    for i, p in enumerate(predicted):\n",
        "        if p in actual and p not in predicted[:i]:\n",
        "            num_hits += 1.0\n",
        "            score += num_hits / (i + 1.0)\n",
        "    return score / min(len(actual), k)\n",
        "\n",
        "def mapk(actual, predicted, k=10):\n",
        "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n"
      ],
      "metadata": {
        "id": "6CusXsspSVCw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  MAP@K calculation with adjusted threshold\n",
        "def prepare_map_data(df, threshold=0.3):  # Lowered threshold\n",
        "    actual = [[i for i, score in enumerate(df['Similarity_Score']) if score > threshold]]\n",
        "    predicted = [sorted(range(len(df)), key=lambda i: df['Predicted_Similarity'].iloc[i], reverse=True)]\n",
        "    return actual, predicted"
      ],
      "metadata": {
        "id": "E8i5VnehSs1z"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate MAP@K for different K values\n",
        "for k in [1, 3, 5, 10]:\n",
        "    actual, predicted = prepare_map_data(ground_truth)\n",
        "    map_score = mapk(actual, predicted, k=k)\n",
        "    print(f\"MAP@{k}: {map_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyymgM4IS65a",
        "outputId": "c34d8009-5498-4359-f759-d242b37b8172"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAP@1: 1.0\n",
            "MAP@3: 0.5555555555555555\n",
            "MAP@5: 0.4833333333333333\n",
            "MAP@10: 0.5325396825396825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions for the test set\n",
        "test_pairs['Predicted_Similarity'] = test_pairs.apply(lambda row: calculate_similarity(row['ID_1'], row['ID_2']), axis=1)\n",
        "\n",
        "# Create submission file\n",
        "submission = test_pairs[['ID_1', 'ID_2', 'Predicted_Similarity']]\n",
        "submission = submission.rename(columns={'Predicted_Similarity': 'Similarity_Score'})\n",
        "submission.to_csv('submission_final.csv', index=False)\n"
      ],
      "metadata": {
        "id": "x621CgiwTCzs"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I3JS2HOxTeXB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}